{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP Performance Comparisons on Different Size of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "+ CP performance: measured by the **coverage rate** and the **average size of prediction intervals**\n",
    "+ Dataset: CIFAR-100\n",
    "+ Comparison candidates: different combinations of 4 score functions (`THR`, `APS`, `SAPS(0.2)`, `RAPS(1,0)`) and 3 predictors (`SplitPredictor`, `ClusterPredictor`, `ClassWisePredictor`).\n",
    "+ Size variation: sample 10 subsets with 10 classes, 20 classes, ..., 100 classes from CIFAR-100 respectively, then conduct CP on each of these data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Define the transformation for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # VGG16 expects 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-100 dataset\n",
    "cifar100_trainset = CIFAR100(root='./data', train=True, transform=transform, download=True)\n",
    "cifar100_testset = CIFAR100(root='./data', train=False, transform=transform, download=True)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-map the class labels to fit the input format of the loss function \n",
    "class RemapCIFAR100(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, selected_classes=None, transform=None, target_transform=None, download=False):\n",
    "        self.cifar100 = datasets.CIFAR100(root, train=train, transform=None, target_transform=None, download=download)\n",
    "        \n",
    "        if selected_classes is None:\n",
    "            selected_classes = list(range(10))  # Default: classes 0 to 9\n",
    "        \n",
    "        self.selected_classes = selected_classes\n",
    "        self.class_mapping = {class_id: idx for idx, class_id in enumerate(selected_classes)}\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Remap labels during initialization\n",
    "        self.data, self.targets = self.remap_labels(self.cifar100.data, self.cifar100.targets)\n",
    "\n",
    "    def remap_labels(self, data, targets):\n",
    "        remapped_targets = [self.class_mapping[target] for target in targets if target in self.selected_classes]\n",
    "        remapped_data = [data[i] for i, target in enumerate(targets) if target in self.selected_classes]\n",
    "\n",
    "        return remapped_data, remapped_targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Subsets Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gen_subset_sequence(seed = 820):\n",
    "    random.seed(seed)\n",
    "    indices = list(range(100))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    subsets = []\n",
    "    for i in range(10):\n",
    "        selected_classes = indices[:10*(i+1)]\n",
    "        # Example usage:\n",
    "\n",
    "        train_dataset = RemapCIFAR100(root='./data', train=True, selected_classes=selected_classes, transform=transforms.ToTensor(), download=True)\n",
    "        test_dataset = RemapCIFAR100(root='./data', train=False, selected_classes=selected_classes, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        subsets.append((selected_classes, train_dataset, test_dataset, train_loader, test_loader))\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_ID = 9    # the size of subset, \"=k\" refers to a subset of 10(k+1) classes k = 0,1,2,...,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vgg16(train_loader, selected_classes):\n",
    "    # Load the pre-trained VGG16 model\n",
    "    vgg16_model = models.vgg16(pretrained=True)\n",
    "\n",
    "    # Modify the model for the new task\n",
    "    vgg16_model.classifier[-1] = nn.Linear(4096, len(selected_classes))  # Adjust the number of classes\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(vgg16_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 1\n",
    "    vgg16_model = vgg16_model.to(device)\n",
    "    # train_loader = train_loader.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        vgg16_model.train()\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = vgg16_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return vgg16_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/1: 100%|██████████| 782/782 [02:01<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "subsets = gen_subset_sequence()\n",
    "subset = subsets[SUBSET_ID]\n",
    "\n",
    "selected_classes, train_dataset, test_dataset, train_loader, test_loader = subset\n",
    "model = train_vgg16(train_loader, selected_classes)\n",
    "torch.save(model.state_dict(), \"models/model_{}0.pth\".format(SUBSET_ID+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP and Obtain the CP Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model If Trained and Saved Previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsets = gen_subset_sequence()\n",
    "# subset = subsets[SUBSET_ID]\n",
    "# selected_classes, train_dataset, test_dataset, train_loader, test_loader = subset\n",
    "# # Load the saved model\n",
    "# model = models.vgg16(pretrained=False)\n",
    "# model.classifier[-1] = nn.Linear(4096, (SUBSET_ID+1)*10)  # Adjust the number of classes\n",
    "# model.load_state_dict(torch.load(\"models/model_{}0.pth\".format(SUBSET_ID+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcp.classification.scores import THR, APS, SAPS, RAPS\n",
    "from torchcp.classification.predictors import SplitPredictor, ClusterPredictor, ClassWisePredictor\n",
    "\n",
    "SCORE_FUNCTIONS = [THR(), APS(), SAPS(0.2), RAPS(1,0)]\n",
    "\n",
    "PREDICTORS = [SplitPredictor, ClusterPredictor, ClassWisePredictor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment--Data : CIFAR-100, Score : THR, Predictor : SplitPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8777, 'Average_size': 6.0148}\n",
      "Experiment--Data : CIFAR-100, Score : THR, Predictor : ClusterPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8757, 'Average_size': 5.9401}\n",
      "Experiment--Data : CIFAR-100, Score : THR, Predictor : ClassWisePredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8746, 'Average_size': 6.4652}\n",
      "Experiment--Data : CIFAR-100, Score : APS, Predictor : SplitPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8764, 'Average_size': 9.9199}\n",
      "Experiment--Data : CIFAR-100, Score : APS, Predictor : ClusterPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8841, 'Average_size': 10.1735}\n",
      "Experiment--Data : CIFAR-100, Score : APS, Predictor : ClassWisePredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8788, 'Average_size': 9.9169}\n",
      "Experiment--Data : CIFAR-100, Score : SAPS, Predictor : SplitPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8781, 'Average_size': 7.067}\n",
      "Experiment--Data : CIFAR-100, Score : SAPS, Predictor : ClusterPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8778, 'Average_size': 7.0735}\n",
      "Experiment--Data : CIFAR-100, Score : SAPS, Predictor : ClassWisePredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8768, 'Average_size': 7.7485}\n",
      "Experiment--Data : CIFAR-100, Score : RAPS, Predictor : SplitPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8796, 'Average_size': 8.0054}\n",
      "Experiment--Data : CIFAR-100, Score : RAPS, Predictor : ClusterPredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8803, 'Average_size': 8.025}\n",
      "Experiment--Data : CIFAR-100, Score : RAPS, Predictor : ClassWisePredictor, Alpha : 0.1\n",
      "{'Coverage_rate': 0.8788, 'Average_size': 8.229}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rows = [score.__class__.__name__ for score in SCORE_FUNCTIONS]\n",
    "columns = [predictor(THR(), model).__class__.__name__ for predictor in PREDICTORS]\n",
    "\n",
    "cov_rates = pd.DataFrame(index=rows, columns=columns)\n",
    "avg_sizes = pd.DataFrame(index=rows, columns=columns)\n",
    "for score in SCORE_FUNCTIONS: \n",
    "    for class_predictor in PREDICTORS:\n",
    "        predictor = class_predictor(score, model)\n",
    "        predictor.calibrate(train_loader, alpha=0.1)\n",
    "        cp_measures = predictor.evaluate(test_loader)\n",
    "        print(f\"Experiment--Data : CIFAR-100, Score : {score.__class__.__name__}, Predictor : {predictor.__class__.__name__}, Alpha : {0.1}\")\n",
    "        print(cp_measures)\n",
    "        cov_rates.loc[score.__class__.__name__][predictor.__class__.__name__] = cp_measures[\"Coverage_rate\"]\n",
    "        avg_sizes.loc[score.__class__.__name__][predictor.__class__.__name__] = cp_measures[\"Average_size\"]\n",
    "\n",
    "cov_rates.to_csv(\"results/cov_rates_{}0.csv\".format(SUBSET_ID+1))\n",
    "avg_sizes.to_csv(\"results/avg_sizes_{}0.csv\".format(SUBSET_ID+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
